import multiprocessing
import os
import asyncio
from datetime import datetime

from playwright.async_api import async_playwright
from portal_processor import AsyncPortalProcessor  # Importar la clase mejorada
from utils import cleanup_directories


async def _async_worker(client_data, target_date, client_download_dir):
    """
    Funci√≥n AS√çNCRONA que contiene la l√≥gica de Playwright con manejo mejorado de errores.
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, slow_mo=50)
        context = await browser.new_context(accept_downloads=True)
        page = await context.new_page()

        try:
            credentials = {
                "nit": client_data["nit"],
                "password": client_data["password"],
            }

            processor = AsyncPortalProcessor(
                page=page,
                main_url=client_data["url"],
                credentials=credentials,
                target_date=target_date,
                base_download_dir=client_download_dir
            )

            # Ahora run() retorna un diccionario con PDFs y errores
            results = await processor.run()

            # Determinar el estado basado en los resultados
            if results["errores"]:
                if results["pdfs_descargados"]:
                    status = "partial_success"
                    print(
                        f"‚ö†Ô∏è Completado parcialmente: {client_data['name']} - {len(results['pdfs_descargados'])} PDFs descargados, {len(results['errores'])} errores")
                else:
                    status = "failed"
                    print(f"‚ùå Fall√≥: {client_data['name']} - {len(results['errores'])} errores")
            else:
                status = "success"
                print(
                    f"‚úÖ Completado exitosamente: {client_data['name']} - {len(results['pdfs_descargados'])} PDFs descargados")

            return {
                "url": client_data['url'],
                "name": client_data['name'],
                "results": results,
                "status": status
            }

        except Exception as e:
            print(f"‚ùå ERROR cr√≠tico con {client_data['name']}: {e}")
            return {
                "url": client_data['url'],
                "name": client_data['name'],
                "results": {
                    "pdfs_descargados": [],
                    "errores": [{
                        "tipo": "ERROR_CRITICO_GENERAL",
                        "mensaje": str(e)
                    }]
                },
                "status": "critical_error"
            }
        finally:
            # Asegurarse de cerrar todo correctamente
            await page.close()
            await context.close()
            await browser.close()


def process_single_client(client_data, target_date, base_download_directory):
    """
    Funci√≥n S√çNCRONA que se ejecutar√° en paralelo.
    """
    client = client_data
    client_download_dir = os.path.join(base_download_directory, client['dir_name'])

    print(f"üöÄ Iniciando procesamiento de {client['name']} en proceso {os.getpid()}")

    try:
        # Tareas s√≠ncronas
        cleanup_directories(client_download_dir)

        # Ejecutar el worker as√≠ncrono
        return asyncio.run(_async_worker(client, target_date, client_download_dir))

    except Exception as e:
        url = client["url"]
        print(f"‚ùå ERROR cr√≠tico con {client['name']}: {e}")
        return {
            "url": url,
            "name": client['name'],
            "results": {
                "pdfs_descargados": [],
                "errores": [{
                    "tipo": "ERROR_CRITICO_PROCESO",
                    "mensaje": str(e)
                }]
            },
            "status": "critical_error"
        }


def print_detailed_summary(results_list):
    """
    Imprime un resumen detallado y formateado de los resultados.
    """
    print("\n\n" + "=" * 80)
    print("                    RESUMEN DETALLADO DE EJECUCI√ìN")
    print("=" * 80)
    print(f"Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("-" * 80)

    total_pdfs = 0
    total_errors = 0
    successful_portals = 0
    failed_portals = 0
    partial_portals = 0

    for result in results_list:
        portal_name = result["name"]
        status = result["status"]
        pdfs = result["results"]["pdfs_descargados"]
        errors = result["results"]["errores"]

        print(f"\nüìä Portal: {portal_name}")
        print(f"   URL: {result['url']}")
        print(f"   Estado: ", end="")

        if status == "success":
            print("‚úÖ EXITOSO")
            successful_portals += 1
        elif status == "partial_success":
            print("‚ö†Ô∏è PARCIALMENTE EXITOSO")
            partial_portals += 1
        elif status == "failed":
            print("‚ùå FALLIDO")
            failed_portals += 1
        else:
            print("üíÄ ERROR CR√çTICO")
            failed_portals += 1

        print(f"   PDFs descargados: {len(pdfs)}")
        total_pdfs += len(pdfs)

        if pdfs:
            print("   Archivos:")
            for pdf in pdfs[:5]:  # Mostrar m√°ximo 5 PDFs
                print(f"      - {pdf['centro']}/{pdf['archivo']}")
            if len(pdfs) > 5:
                print(f"      ... y {len(pdfs) - 5} m√°s")

        if errors:
            print(f"   ‚ö†Ô∏è Errores encontrados: {len(errors)}")
            total_errors += len(errors)
            for error in errors:
                print(f"      - Tipo: {error['tipo']}")
                if 'centro' in error:
                    print(f"        Centro: {error['centro']}")
                print(f"        Mensaje: {error['mensaje']}")
                print()

    print("\n" + "=" * 80)
    print("                         RESUMEN GLOBAL")
    print("=" * 80)
    print(f"Total de portales procesados: {len(results_list)}")
    print(f"  - Exitosos: {successful_portals}")
    print(f"  - Parcialmente exitosos: {partial_portals}")
    print(f"  - Fallidos: {failed_portals}")
    print(f"\nTotal de PDFs descargados: {total_pdfs}")
    print(f"Total de errores: {total_errors}")
    print("=" * 80)


def main():
    # --- Configuraci√≥n General ---
    target_date = "2025/04/25"
    base_download_directory = "notes_downloads"
    MAX_CONCURRENT_CLIENTS = 4  # M√°ximo de clientes en paralelo

    # Lista de portales a procesar
    portal_clients = [
        {
            "nit": "891303109",
            "password": "Cartera2023",
            "name": "Surtifamiliar",
            "url": "https://portalproveedores.surtifamiliar.com",
            "dir_name": "SURTIFAMILIAR",
        },
        {
            "nit": "891303109",
            "password": "2023",
            "name": "Megatiendas",
            "url": "https://proveedores.megatiendas.co/megatiendas",
            "dir_name": "MEGATIENDAS",
        },
        {
            "nit": "891303109",
            "password": "l18mr7",
            "name": "Mercar",
            "url": "http://190.85.196.187",
            "dir_name": "MERCAR",
        },
        {
            "nit": "891303109",
            "password": "Cartera2023",
            "name": "La Monta√±a",
            "url": "https://proveedores.lamontana.co",
            "dir_name": "LAMONTANA",
        },
        # {
        #     "nit": "891303109",
        #     "password": "Toning2020$",
        #     "name": "El Jardin",
        #     "url": "https://proveedores.eljardin.co",
        #     "dir_name": "ELJARDIN",
        # },
    ]

    print(
        f"üéØ Procesando {len(portal_clients)} portales con m√°ximo {MAX_CONCURRENT_CLIENTS} en paralelo usando multiprocessing.Pool")
    print(f"üìÖ Fecha objetivo: {target_date}\n")

    # Preparar los argumentos para cada tarea
    tasks = [(client, target_date, base_download_directory) for client in portal_clients]

    # --- EJECUCI√ìN PARALELA CON MULTIPROCESSING ---
    with multiprocessing.Pool(processes=MAX_CONCURRENT_CLIENTS) as pool:
        results_list = pool.starmap(process_single_client, tasks)

    # Mostrar resumen detallado
    print_detailed_summary(results_list)

    # Guardar resultados en un archivo para referencia
    import json
    results_file = f"logs/resultados_scraping_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_list, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ Resultados guardados en: {results_file}")


if __name__ == "__main__":
    main()